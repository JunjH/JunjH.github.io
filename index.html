<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Junjie Hu, Junjie Hu, CUHK-Shenzhen, The Chinese University of Hong Kong (Shenzhen), Tohoku University"> 
<meta name="description" content="Junjie Hu&#39;s home page">

<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Junjie Hu&#39;s Homepage</title>
<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>

<body>
<div id="layout-content" style="margin-top:25px">
 <a href="https://github.com/JunjH" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#FD6C6C; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

<table>
	<tbody>
		<tr>
			<td width="800">
				<div id="toptitle">					
					<h1>Junjie Hu (胡君杰)</h1><h1>
				</h1></div>

				<h3>Shenzhen Institute of Artificial Intelligence and Robotics for Society </h3>
				<p>
				    
					Shenzhen, Guangdong, China<br>
					<br>
	
					<em>Email: <a href="hujunjie@cuhk.edu.cn">hujunjie@cuhk.edu.cn</a></em> <br>
	
				</p>
				<p> 
					<a href="https://scholar.google.com/citations?user=nuZZKu4AAAAJ&hl=zh-CN&citsig=AMD79ooexm83vBNr71MzBLchUnheziYZLA"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/JunjH"><img src="./pic/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://www.researchgate.net/profile/Junjie-Hu-7"><img src="./pic/rg.png" height="30px" style="margin-bottom:-3px"></a>					
				</p>
			</td>
<!-- 			<td>
				<img src="./pic/hjj.jpg" border="0" width="280"><br>
			</td> -->
		</tr><tr>
	</tr></tbody>
</table>

<h2>Biography </h2>
<p>
	I am currently a researcher at <a href="https://airs.cuhk.edu.cn/en/"> the Shenzhen Institute of Artificial Intelligence and Robotics for Society</a> and an adjunct assistant professor at the School of Science and Engineering, the Chinese University of Hong Kong, Shenzhen. I got my Ph.D. in the 
	<a href="https://www.is.tohoku.ac.jp/en/">Graduate School of Information Sciences</a>, 
	<a href="http://www.tohoku.ac.jp/en/"> Tohoku University</a>, 
	supervised by <a href="https://scholar.google.com/citations?user=gn780jcAAAAJ&hl=zh-CN">Prof. Takayuki Okatani</a> in 2020.
My current research interest lies at the intersection of artificial intelligence and robotics, mainly including multi-sensory learning, lifelong learning, 3D vision, robot perception, and multi-robot perception. </p>

 <p style="color: red; display: inline;"><b>Please drop me an email if you are interested in an internship.</b> </p> 

</p>


<h2>News</h2>
<ul>
	<li>
		[01/2025] Two papers were accepted by ICRA'25.
	</li>
	<li>
		[12/2024] One paper was accepted by AAAI'25.
	</li>
	<li>
		[06/2024] One paper was accepted by IROS'24.
	</li>
	<li>
		[01/2024] Two papers were accepted by ICRA'24.
	</li>
<!-- 	<li>
		[12/2023] One paper on data-free depth distillation was accepted by KBS'23.
	</li>
	 -->
	<li>
		[10/2023] One paper on lifelong depth estimation was accepted by TNNLS'23.
	</li>
	
	<li>
		[09/2023]  One paper on LiDAR depth completion was accepted by RAL'23.
	</li>
	
	<li>
		[06/2023] Invited talk on scene depth estimation at the School of Electronics and Communication Engineering, Sun Yat-sen University, Shenzhen. Slide is  <a href="https://github.com/JunjH/JunjH.github.io/blob/main/slides/Revisiting%20Scene%20Depth%20Estimation.pdf">here</a>. 
	</li>
	
	<li>
		[06/2023]  One paper on few-shot multi-agent perception was accepted by TPAMI'23.
	</li>
	
	<li>
		[04/2023]  One paper on RGB-thermal based perception was accepted by RAL'23.
	</li>
	
	<li>
		[03/2023] Invited talk on robot perception at the School of Artificial Intelligence, South China Normal University. Slide is  <a href="https://github.com/JunjH/JunjH.github.io/blob/main/slides/AI%20driven%20intelligent%20robots.pdf">here</a>. 
	</li>
	
	<li>
		[03/2023]  One paper on multi-robot SLAM was accepted by ICRA'23.
	</li>

<!-- 	<li>
		[02/2023]  One paper on scene recognition was accepted by TIM'23.
	</li> -->


	<li>
		[02/2023]  One paper on heterogeneous multi-robot catching was accepted by TRO'23.
	</li>

	<li>
		[12/2022] The Survey on depth completion was accepted by TPAMI'22.
	</li>
	
</ul>



<h2>Journal Publications</h2>
<div style="font-size: 12px">
        <strong>Notes:</strong> † denotes Joint first authors and * denotes the corresponding author.
</div>
<ul>

	<li>
		<b>Feature Pyramid Attention Network for Audio-Visual Scene Classification</b>, <br />  
		Liguang Zhou, Yuhongze Zhou, Xiaonan Qi, <b>Junjie Hu</b>, Lun Lam Tin, Yangsheng Xu,  <br /> 
		<i>CAAI Transactions on Intelligence Technology </i>, 2024.  <br />  
		<!-- <a href="https://arxiv.org/pdf/2205.14411">paper</a>	 -->
	</li>
	
	<li>
		<b>Dense Depth Distillation with Out-of-Distribution Simulated Images</b>, <br />  
		<b>Junjie Hu</b>, Chenyou Fan, Mete Ozay, Hualie Jiang, Tin Lun Lam,  <br /> 
		<i>Knowledge-Based Systems </i>(KBS), 2023.  <br />  
		<!-- <a href="https://freeformrobotics.org/wp-content/uploads/2023/12/KBS2023_Hu.pdf">paper</a>	 -->
	</li>
	
	<li>
		<b>Lifelong-MonoDepth: Lifelong Learning for Multi-Domain Monocular Metric Depth Estimation</b>, <br />  
		<b>Junjie Hu</b>, Chenyou Fan, Liguang Zhou, Qing Gao, Honghai Liu, Tin Lun Lam,  <br /> 
		<i>IEEE Transactions on Neural Networks and Learning Systems  </i>(TNNLS), 2023.  <br />  
		<!-- <a href="https://ieeexplore.ieee.org/ielx7/5962385/6104215/10293000/supp1-3323487.pdf?arnumber=10293000"> supplementary file </a>, <a href="https://github.com/JunjH/Lifelong-MonoDepth">code</a>				 -->
	</li>
	 <li> 
      <b>Self-supervised Single-line LiDAR Depth Completion</b>, <br />  
		<b>Junjie Hu</b>, Chenyou Fan, Xiyue Guo, Liguang Zhou, Tin Lun Lam,  <br /> 
		<i> IEEE Robotics and Automation Letters </i>(RAL), 2023.  <br />  
		 <!-- <a href="https://freeformrobotics.org/wp-content/uploads/2023/09/single_lidar_completion_ral_2023.pdf">paper</a> -->
   	</li>

	  <li> 
    <b>Few-Shot Multi-Agent Perception with Ranking-Based Feature Learning</b>, <br />  
		Chenyou Fan, <b>Junjie Hu</b> and Jianwei Huang,  <br /> 
		<i>IEEE Transactions on Pattern Analysis and Machine Intelligence </i>(TPAMI), 2023.  <br />  
		<!-- <a href="https://ieeexplore.ieee.org/abstract/document/10149393">paper</a> -->
   	</li>

	<li>
		<b>Explicit Attention-Enhanced Fusion for RGB-Thermal Perception Tasks</b>, <br />  
		Mingjian Liang†,  <b>Junjie Hu</b>†, Chenyu Bao, Hua Feng, Fuqin Deng, Tin Lun Lam,  <br /> 
		<i>IEEE Robotics and Automation Letters  </i>(RAL), 2023.  <br />  
		<!-- <a href="https://github.com/FreeformRobotics/EAEFNet">code</a> -->
	</li>
	
	<li>
	 	<b>Asymmetric Self-play Enabled Intelligent Heterogeneous Multi-robot Catching System using Deep Multi-agent Reinforcement Learning</b>, <br />  
		Yuan Gao, Junfeng Chen, Xi Chen, Chongyang Wang,  <b>Junjie Hu</b>, Fuqin Deng, Tin Lun Lam, <br />
		<i>IEEE Transactions on Robotics </i>(TRO), 2023.  <br />  
		<!-- <a href="https://www.youtube.com/watch?v=cWRFSNV-MQ8">demo</a>  -->
	</li>
	
	<li>
		<b>Attentional Graph Convolutional Network for Structure-aware Audio-Visual Scene Classification</b>, <br />  
		Liguang Zhou, Yuhongze Zhou, Xiaonan Qi, <b>Junjie Hu</b>, Tin Lun Lam, Yangsheng Xu, <br /> 
		<i>IEEE Transactions on Instrumentation and Measurement  </i>(TIM), 2023. <br />  
		<!-- <a href="https://arxiv.org/pdf/2301.00145.pdf">paper</a> -->
	</li>

	<li>
     <b>Deep Depth Completion from Extremely Sparse Data: A Survey</b>, <br />  
		<b>Junjie Hu</b>, Chenyu Bao, Mete Ozay, Chenyou Fan, Qing Gao, Honghai, Liu and Tin Lun Lam, <br /> 
		<i>IEEE Transactions on Pattern Analysis and Machine Intelligence  </i>(TPAMI), 2022.  <br />  
		<!-- <a href="https://freeformrobotics.org/wp-content/uploads/2022/12/TPAMI-2022-08-1659.pdf">paper</a>, <a href="https://ieeexplore.ieee.org/ielx7/34/10144448/9984942/supp1-3229090.pdf?arnumber=9984942"> supplementary file </a> -->
 	</li>

 	<li>
     <b>BaSICNet: Lightweight 3D Hand Pose Estimation Network Based on Biomechanical Structure Information for Dexterous Manipulator Teleoperation</b>, <br />  
		Wenrao Pang, Qing Gao, Yinan Zhao, Zhaojie Ju, <b>Junjie Hu</b>, <br /> 
		<i>IEEE Transactions on Cognitive and Developmental Systems </i>, 2022.  <br />  
		<!-- <a href="https://ieeexplore.ieee.org/abstract/document/9993800">paper</a> -->
 	</li>

 	<li>
      <b>A Two-stage Unsupervised Approach for Low light Image Enhancement</b>, <br />  
		 <b>Junjie Hu</b>, Xiyue Guo, Junfeng Chen, Guanqi Liang, Fuqin Deng and Tin Lun Lam, <br /> 
		<i>IEEE Robotics and Automation Letters  </i>(RAL), 2021. <br />  
		<!-- <a href="https://freeformrobotics.org/wp-content/uploads/2022/02/20-1966_03_MS.pdf">paper</a>, <a href="https://drive.google.com/file/d/1yHhiGUZg7hofS0uKHVERVkdnpyLxRiTT/view?usp=sharing">code</a> -->
 	</li>

  	<li>
     	<b>Semantic Histogram Based Graph Matching for Real-Time Multi-Robot Global Localization in Large Scale Environment</b>, <br />  
		Xiyue Guo, <b>Junjie Hu</b>, Junfeng Chen, Fuqin Deng and Tin Lun Lam, <br /> 
		<i>IEEE Robotics and Automation Letters  </i>(RAL), 2021. <br />  
		<!-- <a href="https://freeformrobotics.org/wp-content/uploads/2022/02/20-2152_03_MS.pdf">paper</a>, <a href="https://www.youtube.com/watch?v=xB8WHj8K9cE">demo</a>, <a href="https://github.com/gxytcrc/semantic-histogram-based-global-localization">code</a> -->
  	</li>

    	<li>   
    	<b>Extending Information Maximization from a Rate--Distortion Perspective</b>, <br />  
		Yan Zhang, <b>Junjie Hu</b> and Takayuki Okatani, <br /> 
		<i>Neurocomputing </i>, 2020, <br />  
		
	</li>

    	<li> 
       <b>PPM-SSNet: An Pyramid Pooling Module-based Semi-Siamese Network for End-to-end Building Damage Assessment from Satellite Imagery</b>, <br />  
		Yanbing Bai, <b>Junjie Hu</b>, Jinhua Su, Xing Liu, Haoyu Liu, Xianwen He, Shengwang Meng, Erick Mas, Shunichi Koshimura, <br />  
		<i>Remote Sensing</i>, 2020. <br />  
		<!-- <a href="https://www.mdpi.com/2072-4292/12/24/4055">paper</a> -->
	</li>

</ul>

<h2>Conference Publications</h2>
<div style="font-size: 12px">
        <strong>Notes:</strong> † denotes Joint first authors and * denotes the corresponding author.
</div>
<ul>
	<li>
		<b>Topology-based Visual Active Room Segmentation</b>, <br />  
		Chenyu Bao†, <b>Junjie Hu</b>†, Qiu Zheng, Tin Lun Lam, <br /> 
		<i>IEEE International Conference on Robotics and Automation </i> (ICRA), 2025. <br />  
  	</li>
	<li>
		<b>Transferring Visual Knowledge: Semi-Supervised Instance Segmentation for Object Navigation Across Varying Height Viewpoints</b>, <br />  
		Qiu Zheng†, <b>Junjie Hu</b>†, Yuming Liu, Zengfeng Zeng, Wang Fan, Tin Lun Lam, <br /> 
		<i>IEEE International Conference on Robotics and Automation </i> (ICRA), 2025. <br />  
  	</li>

	<li>
		<b>DualNet: Robust Self-supervised Stereo Matching with Pseudo-label Supervision</b>, <br />  
		Yun Wang, Jiahao Zheng, Chenghao Zhang, Zhanjie Zhang, Kunhong Li, Yongjian Zhang, <b>Junjie Hu</b>*, <br /> 
		<i>The 39th Annual AAAI Conference on Artificial Intelligence </i> (AAAI), 2025. <br />  
  	</li>	
	<li>
		<b>Streamlining Forest Wildfire Surveillance: AI-Enhanced UAVs Utilizing the FLAME Aerial Video Dataset for Lightweight and Efficient Monitoring</b>, <br />  
		Lemeng Zhao, <b>Junjie Hu</b>, Jianchao Bi, Yanbing Bai, Mas Erick, Shunichi Koshimura, <br /> 
		<i>IEEE/RSJ International Conference on Intelligent Robots and Systems </i> (IROS), 2024. <br />  
  	</li>	

	
	<li>
		<b>Evaluating Performance of LLaMA2 Large Language Model Enhanced by QLoRA Fine-Tuning for English Grammatical Error Correction</b>, <br />  
		Jing An, Yanbing Bai, Jiyi Li,  <b>Junjie Hu</b>, Rui Li, Yuxi Xiao, Rui Hua, <br /> 
		<i>International Conference on Database and Expert Systems Applications  </i> (DEXA), 2024. <br />  
  	</li>	

	
	<li>
		<b>From Satellite to Ground: Satellite Assisted Visual Localization with Cross-view Semantic Matching</b>, <br />  
		Xiyue Guo, Haocheng Peng, <b>Junjie Hu</b>, Hujun Bao, Guofeng Zhang, <br /> 
		<i>IEEE International Conference on Robotics and Automation </i> (ICRA), 2024. <br />  
  	</li>	
	
	<li>
		<b>Meta-Reinforcement Learning Based Cooperative Surface Inspection of 3D Uncertain Structures using Multi-robot Systems</b>, <br />  
		Junfeng Chen, Yuan Gao, <b>Junjie Hu</b>, Fuqing Deng, Tin Lun Lam, <br /> 
		<i>IEEE International Conference on Robotics and Automation </i> (ICRA), 2024. <br />  
		<!-- <a href="https://arxiv.org/pdf/2109.13617.pdf">paper</a> -->
  	</li>	
	
	<li>
		<b>MultiRoboLearn: An open-source Framework for Multi-robot Deep Reinforcement Learning</b>, <br />  
		Junfeng Chen, Fuqin Deng, Yuan Gao,  <b>Junjie Hu</b>, Xiyue Guo, Guanqi Liang, Tin Lun Lam, <br /> 
		<i>IEEE International Conference on Robotics and Biomimetics  </i>(ROBIO), 2023. <br />  
		<!-- <a href="https://arxiv.org/pdf/2209.13760.pdf">paper</a>, <a href="https://github.com/JunfengChen-robotics/MultiRoboLearn">code</a> -->
  	</li>	
	
	<li>
		<b>Trajectory prediction with contrastive pre-training and social rank fine-tuning</b>, <br />  
		Chenyou Fan, Haiqi Jiang, Aimin Huang, and  <b>Junjie Hu</b>*, <br />  
		<i>International Conference on Neural Information Processing </i>(ICONIP), 2023. <br />  
		<!-- <a href="https://fanchenyou.github.io/docs/iconip_trajectory_prediction.pdf">paper</a> -->
  	</li>	
	
	<li>
	   <b>Boosting Light-Weight Depth Estimation Via Knowledge Distillation</b>,<br />  
		<b>Junjie Hu</b>, Chenyou Fan, Hualie Jiang, Xiyue Guo, Yuan Gao, Xiangyong Lu, Tin Lun Lam, <br /> 
		<i>The 16th International Conference on Knowledge Science, Engineering and Management </i>(KSEM), 2023. <br />  
		<!-- <a href="https://arxiv.org/pdf/2105.06143.pdf">paper</a>, <a href="https://github.com/JunjH/Boosting-Light-Weight-Depth-Estimation">code</a> -->
  	</li>	

	<li>
	  <b>Descriptor Distillation for Efficient Multi-robot SLAM</b>, <br />  
		Xiyue Guo, <b>Junjie Hu</b>, Hujun Bao, Guofeng Zhang, <br />  
		<i>IEEE International Conference on Robotics and Automation </i>(ICRA), 2023. <br />  
		<!-- <a href="https://arxiv.org/pdf/2303.08420.pdf">paper</a> -->
	</li>

  	<li>   
	   <b>Private Semi-Supervised Federated Learning</b>, <br />  
		Chenyou Fan, <b>Junjie Hu</b>, Jianwei Huang,  <br /> 
		<i>International Joint Conferences on Artificial Intelligence </i>(IJCAI), 2022. <br />  
		<!-- <a href="https://www.ijcai.org/proceedings/2022/0279.pdf">paper</a> -->
 	</li>

   	 <li>  
     	<b> PLNet: Plane and Line Priors for Unsupervised Indoor Depth Estimation</b>, <br />  
		 Hualie Jiang, Laiyan Ding, <b>Junjie Hu</b> and  Rui Huang, <br /> 
		 <i>IEEE International Conference on 3D Vision </i>(3DV), 2021.  <br />  
		 <!-- <a href="https://arxiv.org/pdf/2110.05839.pdf">paper</a>, <a href="https://github.com/HalleyJiang/PLNet">code</a> -->
    	</li>

   	<li> 
     <b>Few-Shot Multi-Agent Perception</b>, <br />  
		Chenyou Fan, <b>Junjie Hu</b> and Jianwei Huang, <br />  
		<i>ACM Multimedia </i>(ACM MM), 2021.   <br />  
		<!-- <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475315">paper</a>, <a href="https://github.com/fanchenyou/fs-map-project">code</a> -->
   	</li>

    	<li>             
     <b>FEANet: Feature-Enhanced Attention Network for RGB-Thermal Real-Time Semantic Segmentation</b>, <br />  
		Fuqin Deng, Hua Feng, Mingjian Liang, Hongmin Wang, Yong Yang, Yuan Gao, Junfeng Chen, <b>Junjie Hu</b>, Xiyue Guo, Tin Lun Lam, <br /> 
		<i>IEEE/RSJ International Conference on Intelligent Robots and Systems </i>(IROS), 2021. <br />  
		<!-- <a href="https://freeformrobotics.org/wp-content/uploads/2022/02/IROS21_2007_FI.pdf">paper</a>, <a href="https://github.com/FreeformRobotics/FEANet">code</a> -->
    	</li>

    	<li> 
     		<b>Visualization of Convolutional Neural Networks for Monocular Depth Estimation</b>, <br />  
	    	<b>Junjie Hu</b>, Yan Zhang and Takayuki Okatani, <br />
	    	<i>IEEE International Conference on Computer Vision  </i>(ICCV), 2019. <br />  
	    	<!-- <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Visualization_of_Convolutional_Neural_Networks_for_Monocular_Depth_Estimation_ICCV_2019_paper.pdf">paper</a>, <a href="https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Hu_Visualization_of_Convolutional_ICCV_2019_supplemental.pdf"> supplementary file </a>, <a href="https://github.com/JunjH/Visualizing-CNNs-for-monocular-depth-estimation">code</a> -->
    	</li>

    	<li> 
   		<b>Revisiting Single Image Depth Estimation: Toward Higher Resolution Maps with Accurate Object Boundaries</b>,<br />  
		<b>Junjie Hu</b>, Mete Ozay, Yan Zhang and Takayuki Okatani,<br />  
		<i>IEEE Winter Conference on Applications of Computer Vision </i>(WACV), 2019.<br />  
		<!-- <a href="https://arxiv.org/pdf/1803.08673.pdf">paper</a>, <a href="https://github.com/JunjH/Revisiting_Single_Depth_Estimation">code</a> -->
   	</li>

    	<li> 
     		<b>Non-rigid structure from motion via sparse self-expressive representation</b>,<br />  
		<b>Junjie Hu</b> and Terumasa Aoki,<br /> 
		<i>IEEE International Conference on Image Processing </i>(ICIP), 2017.<br />  
		<!-- <a href="https://ieeexplore.ieee.org/abstract/document/8297141">paper</a> -->
	</li>

    	<li> 
     <b>A Convex Approach for Non-rigid Structure from Motion Via Sparse Representation</b>,<br />  
		<b>Junjie Hu</b> and Terumasa Aoki,<br />  
		<i>International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications </i>(VISIGRAPP), 2017.<br />  
		<!-- <a href="https://pdfs.semanticscholar.org/5355/40fd53ed3a04f6d45e22e791de75c7208d8f.pdf">paper</a> -->
	</li>

    	<li> 
      <b>Improving the performance of non-rigid 3D shape recovery by points classification</b>,<br />  
		<b>Junjie Hu</b> and Terumasa Aoki,<br /> 
		<i>IEEE International Conference on Machine Vision Applications </i>(MVA), 2017. <br />  
		<!-- <a href="https://www.mva-org.jp/Proceedings/2017USB/papers/09-09.pdf">paper</a> -->
	</li>	
</ul>


<h2>Honors &amp; Awards</h2>
<ul>
	
	<li>	
		<tr><td> Employee of the year, Shenzhen Institute of Artificial Intelligence and Robotics for Society, 2023</td></tr> 
	</li>
	
	<li>
		<tr><td> Shenlong High-level Talent, Level C, Longgang district, Shenzhen city, 2021</td></tr>
	</li>
	
	<li>
		<tr><td> Overseas High-Caliber Personnel, Level C, Shenzhen City, 2020</td></tr>
	</li>
	
	<li>	
		<tr><td> Best presentation award, Graduate School of Information Sciences, Tohoku University, 2019</td></tr> 
	</li>

	<li>	
		<tr><td> Global Hagi Scholarship, Tohoku University, 2018 - 2020</td></tr> 
	</li>

	<li>	
		<tr><td> Rotary Yoneyama Scholarship, 2016 - 2017</td></tr> 
	</li>

	</tbody>
</ul>


<h2>Professional Activities</h2>
<ul>
	<li>	
	<b>Conference Reviews:</b><br>
	IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)<br>
	IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)<br>
	IEEE International Conference on Robotics and Automation (ICRA)<br>
	European Conference on Computer Vision (ECCV)<br>		
	</li>

	<li>	
	<b>Journal Reviews:</b><br>
	International Journal of Computer Vision (IJCV) <br>
	IEEE Transactions on Image Processing (TIP) <br>
	IEEE Transactions on Neural Network and Learning Systems (TNNLS) <br>
	IEEE Robotics and Automation Letters (RAL) <br>
	IEEE Transactions on Instrumentation and Measurement (TIM) <br>
	IEEE Transactions on Mechatronics (TMECH) <br>
	Journal of Field Robotics (JFR) <br>
	Knowledge-Based Systems (KBS) <br>
	Neurocomputing <br>
	<p style="margin-top:3px"></p>		
	</li>
</ul>
	<br>
	<!-- <a href='https://clustrmaps.com/site/1bvo7'  title='Visit tracker'><center><img src="//clustrmaps.com/globe.js?d=Z5m70X6XaRydMUp1bTMJMkgQLNH5r5EEuAAvZUceaR0"/></center></a> -->
	
	<div style="height: 250px;width: 250px;margin-left: 40px;">
  	<script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=Z5m70X6XaRydMUp1bTMJMkgQLNH5r5EEuAAvZUceaR0"></script>
	</div>

</body></html>
